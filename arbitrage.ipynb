{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Columbia University E4620 Final Project\n",
    "Author @Alex Wei, Last modified: Dec. 13, 2025"
   ],
   "id": "84cc41e9b2d28a94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, root_mean_squared_error as rms, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"font.family\"] = \"Arial\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DIR = Path('./datasets')\n",
    "RATE_L, TARIFF = 26.67, 15.0        # converted to USD\n",
    "IMP_THRESH, EXP_THRESH = 15.0, -5.0 # economic thresholds for IO decisions\n",
    "CAPACITY = 1250                     # assume same as Hertel-NY\n",
    "\n",
    "def modified_gram_schmidt(A):   # copied from my HW3Q5\n",
    "    m, n = A.shape\n",
    "    Q, R, V = np.zeros((m,n)), np.zeros((n,n)), A.copy().astype(float)\n",
    "    for j in range(n):\n",
    "        for i in range(j):\n",
    "            R[i, j] = Q[:, i] @ V[:, j]\n",
    "            V[:, j] -= R[i, j] * Q[:, i]\n",
    "        R[j, j] = np.linalg.norm(V[:, j])\n",
    "        Q[:, j] = V[:, j] / R[j, j] if R[j, j] > 1e-12 else V[:, j]\n",
    "    return Q, R\n",
    "\n",
    "def ridge_qr(X, y, lam):\n",
    "    X_aug = np.vstack([X, np.sqrt(lam) * np.eye(X.shape[1])])\n",
    "    y_aug = np.concatenate([y, np.zeros(X.shape[1])])\n",
    "    Q, R = modified_gram_schmidt(X_aug)\n",
    "    return np.linalg.solve(R, Q.T @ y_aug)\n",
    "\n",
    "def hard_impute(X, rank, max_iter=50, tol=1e-4):    # REF [1]\n",
    "    X_f = X.copy()  # filled\n",
    "    msk = np.isnan(X)\n",
    "    if not np.any(msk):\n",
    "        return X_f\n",
    "    row_means = np.nanmean(X, axis=1)\n",
    "    for i in range(X.shape[0]):\n",
    "        X_f[i, msk[i, :]] = (row_means[i] if not np.isnan(row_means[i]) else np.nanmean(X))\n",
    "    for _ in range(max_iter):\n",
    "        X_old = X_f.copy()\n",
    "        U, s, Vt = np.linalg.svd(X_f, full_matrices=False)\n",
    "        s[rank:] = 0\n",
    "        X_f[msk] = (U @ np.diag(s) @ Vt)[msk]\n",
    "        if np.linalg.norm(X_f - X_old, 'fro') < tol:\n",
    "            break\n",
    "    return X_f"
   ],
   "id": "2a0d438f93d49a75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data loading",
   "id": "aa7a371602b6c930"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.DataFrame(index=pd.date_range('2015-01-01', '2024-12-31 23:00:00', freq='h'))\n",
    "\n",
    "for dtype, folder, names in [('RT LBMP', 'NYISO_RT_LBMP', ['N.Y.C.', 'WEST']),\n",
    "                             ('DA LBMP', 'NYISO_DA_LBMP', ['N.Y.C.']),\n",
    "                             ('Loads', 'NYISO_Load', ['N.Y.C.'])]:\n",
    "    dfs = [pd.read_csv(f, parse_dates=[0]) for f in tqdm(sorted((DIR/folder).glob('*.csv')), desc=dtype, leave=False)]\n",
    "    dfs = [df[df.iloc[:, 1].isin(names) if dtype != 'Loads' else df['Name'].isin(names)] for df in dfs]\n",
    "    if not dfs: continue\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    if dtype == 'Loads':\n",
    "        data['Load'] = (df.pivot_table(index='Time Stamp', columns='Name', values='Load', aggfunc='mean')['N.Y.C.'].resample('h').mean())\n",
    "    else:\n",
    "        df_pivot = df.pivot_table(index=df.columns[0], columns=df.columns[1], values=df.columns[3], aggfunc='mean')\n",
    "        prefix = 'DA_' if dtype == 'DA LBMP' else ''\n",
    "        for col in df_pivot.columns:\n",
    "            data[f'{prefix}LBMP_{col.replace(\"N.Y.C.\", \"NYC\").replace(\" \", \"_\")}'] = (df_pivot[col].resample('h').mean())\n",
    "\n",
    "# weather data\n",
    "df = pd.read_csv(DIR / 'NOAA/KNYC.csv', parse_dates=['valid'])\n",
    "for col in ['tmpc', 'dwpc', 'sped']:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df_nyc = df.set_index('valid')[['tmpc', 'dwpc', 'sped']].resample('h').mean()\n",
    "data['Temp'], data['NYC_Dewpoint'], data['NYC_Windspeed'] = (df_nyc['tmpc'], df_nyc['dwpc'], df_nyc['sped'])\n",
    "\n",
    "data = data.interpolate(method='time', limit=48).ffill(limit=72).bfill(limit=72)"
   ],
   "id": "b83d92efdd955d0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Feature engineering",
   "id": "cfbbf7f9ad003012"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# HardImpute SVD on load matrix\n",
    "n_days = len(data['Load']) // 24\n",
    "load_matrix = hard_impute(data['Load'].values[:n_days * 24].reshape(n_days, 24).T, rank=3)\n",
    "U, s, Vt = np.linalg.svd(load_matrix, full_matrices=False)\n",
    "d_eig = U[:, :3]\n",
    "\n",
    "data['Load_Completed'] = np.nan\n",
    "data.iloc[:n_days * 24, data.columns.get_loc('Load_Completed')] = load_matrix.T.flatten()\n",
    "data['Load_Completed'] = data['Load_Completed'].bfill().ffill()\n",
    "\n",
    "# eigen day coeffs\n",
    "for i, coef in enumerate((Vt[:3, :] * s[:3, None]).T.T):\n",
    "    data[f'Eigen_{i + 1}'] = np.nan\n",
    "    data.iloc[:n_days*24, data.columns.get_loc(f'Eigen_{i + 1}')] = np.repeat(coef, 24)\n",
    "data[['Eigen_1', 'Eigen_2', 'Eigen_3']] = (data[['Eigen_1', 'Eigen_2', 'Eigen_3']].bfill().ffill())\n",
    "\n",
    "data['Spread'] = data['LBMP_NYC'] - (RATE_L + TARIFF)   # target variable\n",
    "data['DA_Spread'] = data['DA_LBMP_NYC'] - (RATE_L + TARIFF) # main predictor (known 1 day ahead)\n",
    "\n",
    "# price features: use DA and lagged RT for balance\n",
    "data['Price_Range'] = (data['LBMP_NYC'].shift(1).rolling(24, min_periods=12).apply(lambda x: x.max() - x.min()))  # yesterday's volatility\n",
    "data['HDD'] = np.maximum(0, 18.3 - data['Temp'])    # heating degree days\n",
    "data['CDD'] = np.maximum(0, data['Temp'] - 18.3)    # cooling degree days\n",
    "data['Temp_Sq'] = data['Temp'] ** 2                 # nonlinear temp effect\n",
    "\n",
    "for lag in [1, 24, 168]:    # lagged features (mix of DA & RT)\n",
    "    data[f'LBMP_L{lag}'] = data['LBMP_NYC'].shift(lag)  # recent RT price hist\n",
    "    data[f'DA_L{lag}'] = data['DA_LBMP_NYC'].shift(lag) # DA price hist\n",
    "    data[f'Load_L{lag}'] = data['Load_Completed'].shift(lag)\n",
    "\n",
    "for w in [6, 12, 24]:\n",
    "    data[f'LBMP_MA{w}'] = data['LBMP_NYC'].shift(1).rolling(w, min_periods=w // 2).mean()\n",
    "    data[f'DA_MA{w}'] = data['DA_LBMP_NYC'].shift(1).rolling(w, min_periods=w // 2).mean()\n",
    "    data[f'Load_MA{w}'] = data['Load_Completed'].shift(1).rolling(w, min_periods=w // 2).mean()\n",
    "\n",
    "# dynamics & volatility\n",
    "data['LBMP_Trend'] = data['LBMP_NYC'].shift(1) - data['LBMP_NYC'].shift(25)\n",
    "data['DA_Trend'] = data['DA_LBMP_NYC'].shift(1) - data['DA_LBMP_NYC'].shift(25)\n",
    "data['Load_Trend'] = data['Load_Completed'].shift(1) - data['Load_Completed'].shift(25)\n",
    "data['Price_Std'] = data['LBMP_NYC'].shift(1).rolling(24, min_periods=12).std()\n",
    "\n",
    "# temporal features (cyclical patterns)\n",
    "dt_idx = pd.DatetimeIndex(data.index)\n",
    "data['Hour_sin'] = np.sin(2 * np.pi * dt_idx.hour / 24)\n",
    "data['Hour_cos'] = np.cos(2 * np.pi * dt_idx.hour / 24)\n",
    "data['Month_sin'] = np.sin(2 * np.pi * dt_idx.month / 12)\n",
    "data['Month_cos'] = np.cos(2 * np.pi * dt_idx.month / 12)\n",
    "data['IsPeak'] = ((dt_idx.hour >= 7) & (dt_idx.hour <= 21)).astype(int) # peak pricing hours\n",
    "data['IsWeekend'] = (dt_idx.dayofweek >= 5).astype(int)                 # weekend effect\n",
    "\n",
    "# interaction features\n",
    "data['Load_Temp'] = data['Load_L1'] * data['Temp']\n",
    "data['DA_Peak'] = data['DA_Spread'] * data['IsPeak']  # day ahead spread during peak\n",
    "\n",
    "# labels (economic decision boundaries)\n",
    "data['Label'] = 1                                   # HOLD\n",
    "data.loc[data['Spread'] < EXP_THRESH, 'Label'] = 0  # EXP\n",
    "data.loc[data['Spread'] > IMP_THRESH, 'Label'] = 2  # IMP"
   ],
   "id": "f4d0f2f39e99e075",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data splitting + preprocessing",
   "id": "cfe0033bd233a35b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = data.dropna()\n",
    "train = data[data.index < '2022-01-01']\n",
    "val = data[(data.index >= '2022-01-01') & (data.index < '2023-01-01')]\n",
    "test = data[data.index >= '2023-01-01']\n",
    "\n",
    "print(f\"SAMPLES: {len(train)} train | {len(val)} val | {len(test)} test (EXP {sum(test['Label'] == 0)}, HOLD {sum(test['Label'] == 1)}, \"f\"IMP {sum(test['Label'] == 2)})\")\n",
    "print(f\"train period: {train.index[0].strftime('%Y %m %d')} to \"f\"{train.index[-1].strftime('%Y %m %d')}\")\n",
    "print(f\"test period: {test.index[0].strftime('%Y %m %d')} to \"f\"{test.index[-1].strftime('%Y %m %d')}\")\n",
    "\n",
    "feat = [c for c in data.columns if any(x in c for x in\n",
    "                                       ['LBMP_L', 'LBMP_MA', 'LBMP_Trend', 'DA_L', 'DA_MA', 'DA_Spread', 'DA_Trend', 'DA_Peak',\n",
    "                                        'Load_L', 'Load_MA', 'Load_Trend', 'Load_Temp',\n",
    "                                        'HDD', 'CDD', 'Temp_Sq', 'Hour_', 'Month_', 'IsPeak', 'IsWeekend',\n",
    "                                        'Price_Range', 'Price_Std', 'Eigen_', 'NYC_Dewpoint', 'NYC_Windspeed'])]\n",
    "\n",
    "X_train, y_train = train[feat].values, train['Spread'].values\n",
    "X_val, y_val = val[feat].values, val['Spread'].values\n",
    "X_test, y_test = test[feat].values, test['Spread'].values\n",
    "y_test_c = test['Label'].values\n",
    "print(f\"features: {len(feat)}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val, X_test = scaler.transform(X_val), scaler.transform(X_test)"
   ],
   "id": "d3739458f1b7d5eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Regression model training",
   "id": "2fc6aefab6e32849"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lams = np.logspace(-2, 3, 25)\n",
    "val_rmse = [rms(y_val, X_val @ ridge_qr(X_train, y_train, l)) for l in tqdm(lams, desc=\"tuning\", leave=False)]\n",
    "opt_lam = lams[np.argmin(val_rmse)]\n",
    "w = ridge_qr(X_train, y_train, opt_lam)\n",
    "y_pred = X_test @ w\n",
    "\n",
    "print(f\"RMSE {rms(y_test, y_pred):.2f}, R² {r2_score(y_test, y_pred):.3f}\")"
   ],
   "id": "2ceea05c2d3cc75c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Classification via threshold calibration [2]",
   "id": "d24f746177b834f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_val_pred = X_val @ w\n",
    "y_train_pred = X_train @ w\n",
    "bias_correction = np.mean(y_train - y_train_pred)\n",
    "y_val_pred_corrected = y_val_pred + bias_correction\n",
    "y_pred_corrected = y_pred + bias_correction\n",
    "\n",
    "opt_score, th_low, th_high = 0, EXP_THRESH, IMP_THRESH\n",
    "for t_low in np.linspace(-6, -3, 25):       # export must be negative\n",
    "    for t_high in np.linspace(15, 25, 25):\n",
    "        if t_high <= t_low + 12: continue\n",
    "        y_pred_val_c = np.ones(len(y_val_pred_corrected))\n",
    "        y_pred_val_c[y_val_pred_corrected > t_high] = 2\n",
    "        y_pred_val_c[y_val_pred_corrected < t_low] = 0\n",
    "\n",
    "        cm = confusion_matrix(val['Label'].values, y_pred_val_c, labels=[0, 1, 2])\n",
    "        f1 = float(np.mean([2 * cm[k, k] / (cm[:, k].sum() + cm[k, :].sum()) for k in range(3)]))\n",
    "        # normalized profit\n",
    "        volume = np.where(y_pred_val_c == 2, CAPACITY * np.minimum(1, y_val_pred_corrected / 50), 0)\n",
    "        val_profit = np.sum(volume * np.maximum(0, y_val_pred_corrected)) / 1e8\n",
    "\n",
    "        score = 0.8 * f1 + 0.2 * val_profit   # weighted score for balanced optimization\n",
    "        if score > opt_score:\n",
    "            opt_score, th_low, th_high = score, t_low, t_high\n",
    "\n",
    "print(f\"bias correction: {bias_correction:.2f} $/MWh\")\n",
    "print(f\"thresholds: EXP < {th_low:.1f}, HOLD ∈ [{th_low:.1f}, {th_high:.1f}], IMP > {th_high:.1f}\")\n",
    "\n",
    "# apply to test set with bias correction\n",
    "y_pred_c = np.ones(len(y_pred_corrected))\n",
    "y_pred_c[y_pred_corrected > th_high], y_pred_c[y_pred_corrected < th_low] = 2, 0\n",
    "\n",
    "# volume-adjusted profit calculation\n",
    "volume = np.where(y_pred_c == 2, CAPACITY * np.minimum(1, y_pred_corrected / 50), 0)\n",
    "profit = np.cumsum(volume * np.maximum(0, y_pred_corrected))\n",
    "\n",
    "cm = confusion_matrix(y_test_c, y_pred_c)\n",
    "cls = ['Export', 'Hold', 'Import']\n",
    "print(f\"\\ntest acc: {np.mean(y_pred_c == y_test_c):.3f}\")\n",
    "for k, n in enumerate(cls):\n",
    "    tp = cm[k, k]\n",
    "    p = tp / cm[:, k].sum() if cm[:, k].sum() > 0 else 0\n",
    "    r = tp / cm[k, :].sum() if cm[k, :].sum() > 0 else 0\n",
    "    f = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "    print(f\"{n:8s} prec {p:.3f}, rec {r:.3f}, F1 {f:.3f}\")\n",
    "\n",
    "print(f\"\\nPROFIT ANALYSIS\")\n",
    "print(f\"  total profit: ${profit[-1] / 1e6:.2f}M over {len(test)} hrs\")\n",
    "print(f\"  avg profit/hr: ${profit[-1] / sum(y_pred_c == 2):.0f}\")\n",
    "print(f\"  total energy imported: {volume.sum() / 1e6:.3f} TWh\")"
   ],
   "id": "70011eb753729c0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualization",
   "id": "a70a8feefc71bc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "res = y_test - y_pred\n",
    "res_mean, res_std = float(np.mean(res)), float(np.std(res))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.semilogx(lams, val_rmse)\n",
    "plt.axvline(opt_lam, color='r', ls='--', label=f'$\\\\lambda$={opt_lam:.2f}')\n",
    "plt.xlabel('$\\\\lambda$')\n",
    "plt.ylabel('val RMSE')\n",
    "plt.legend()\n",
    "#plt.title('Hyperparam Tuning')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cls, yticklabels=cls)\n",
    "#plt.title('Confusion Matrix')\n",
    "plt.xlabel(\"predicted\")\n",
    "plt.ylabel(\"actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "idx = slice(0, min(336, len(y_test)))\n",
    "plt.plot(y_test[idx], label='actual', lw=1.5)\n",
    "plt.plot(y_pred_corrected[idx], '--', label='pred (bias-corrected)', alpha=0.7)\n",
    "plt.axhline(th_high, color='g', ls=':', alpha=0.7, label=f'IMP > {th_high:.1f}')\n",
    "plt.axhline(th_low, color='orange', ls=':', alpha=0.7, label=f'EXP < {th_low:.1f}')\n",
    "plt.xlabel('hr')\n",
    "plt.ylabel('spread ($/MWh)')\n",
    "plt.legend()\n",
    "#plt.title('Predictions (First 2 Weeks)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "top = np.argsort(np.abs(w))[-10:]\n",
    "plt.barh([feat[i][:15] for i in top], np.abs(w)[top], color='steelblue')\n",
    "plt.xlabel('|coef|')\n",
    "#plt.title('Top Features by Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(7, 8))\n",
    "axes[0].hist(res, bins=50, edgecolor='c')\n",
    "axes[0].axvline(res_mean, color='r', ls='--', label=f'mean={res_mean:.2f}')\n",
    "axes[0].axvline(0, color='k', ls='-', lw=1, alpha=0.3)\n",
    "axes[0].set_xlabel('residual ($/MWh)')\n",
    "axes[0].set_ylabel('count')\n",
    "#axes[0].set_title(f'Residual Distribution ($\\\\sigma$={res_std:.2f})')\n",
    "axes[0].legend()\n",
    "axes[1].set_xlabel('predicted ($/MWh)')\n",
    "axes[1].set_ylabel('residual ($/MWh)')\n",
    "#axes[1].set_title('Residual vs Predicted')\n",
    "axes[1].grid(alpha=0.3)\n",
    "sc = axes[1].scatter(y_pred, res, alpha=0.3, s=5, c=y_test_c, cmap='viridis')\n",
    "axes[1].axhline(res_mean, color='r', ls='--')\n",
    "axes[1].axhline(0, color='k', ls='-', alpha=0.3)\n",
    "fig.colorbar(sc, ax=axes[1], label='class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(profit)), profit / 1e6, 'g-')\n",
    "plt.fill_between(range(len(profit)), 0, profit / 1e6, alpha=0.1, color='g')\n",
    "plt.xlabel('hr')\n",
    "plt.ylabel('profit (million $)')\n",
    "#plt.title(f'Cumulative Profits (total ${profit[-1] / 1e6:.2f}M in 2 years)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 14))\n",
    "for i in range(3):\n",
    "    plt.subplot(3, 1, i + 1)\n",
    "    plt.plot(np.arange(24), d_eig[:, i])\n",
    "    plt.xlabel('hr')\n",
    "    plt.ylabel('amp')\n",
    "    plt.title(f'Eigen Day {i + 1} ({(s[i]**2 / (s**2).sum()) * 100:.2f}% var)')\n",
    "    plt.xticks([0, 6, 12, 18, 24])\n",
    "    plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a3d2d67bbbfa2b92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "[1] Mazumder, R., Hastie, T., & Tibshirani, R. (2010). Spectral regularization algorithms for learning large incomplete matrices. The Journal of Machine Learning Research, 11, 2287-2322.\n",
    "\n",
    "[2] Bessembinder, H., & Lemmon, M. L. (2002). Equilibrium pricing and optimal hedging in electricity forward markets. the Journal of Finance, 57(3), 1347-1382."
   ],
   "id": "8a37867aefac3228"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
